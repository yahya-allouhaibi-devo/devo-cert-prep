---
name: cert-prep-dev-guide
description: Guided development workflow for building the Cert Prep Platform. Generates detailed GitHub issues for moderate-sized tasks (2-4 hours), provides educational code reviews, and teaches best practices. Use when starting a new task ("what should I work on next?"), reviewing completed work ("review my changes"), or needing implementation guidance. Follows a 9-phase progression from foundation to production-ready features.
---

# Cert Prep Platform Development Guide Skill

## Overview
This skill guides you through building the Cert Prep Platform step-by-step, generating detailed GitHub issues for moderate-sized tasks and providing code reviews to ensure you learn best practices while building the project.

## Project Context

### Technology Stack
- **Backend**: FastAPI, SQLModel, Python 3.13, LangGraph, LangChain, AWS Bedrock
- **Frontend**: Nuxt 3, Nuxt UI, Pinia, TypeScript
- **Database**: PostgreSQL
- **Caching**: Redis
- **Deployment**: AWS (Lambda, RDS, etc.)
- **Development**: Docker Compose for local services

### Architecture
Monorepo structure with separate `backend/` and `frontend/` directories.

### Project Goals
Build an internal certification prep platform where IT consultants can:
1. Practice questions with AI-powered explanations
2. Take realistic exam simulations
3. Track progress and identify weak areas
4. Prepare for AWS, Azure, GCP, and Snowflake certifications

## Skill Capabilities

### 1. Generate Next Issue
When the user asks "what should I work on next?" or "generate next issue":

**Process:**
1. Review the project's current state by examining:
   - Existing files and directory structure
   - Git history to understand what's been completed
   - README or project documentation
   - Open/closed GitHub issues (if repo exists)

2. Determine the logical next task based on:
   - Project phase (setup â†’ core features â†’ advanced features)
   - Dependencies (don't create issues for features that require incomplete work)
   - Moderate scope (2-4 hours of work, not too large or too small)

3. Create a detailed GitHub issue using the `gh` CLI with:
   - Clear, descriptive title
   - Problem statement (why this task matters)
   - Acceptance criteria (specific, measurable outcomes)
   - Detailed implementation guidance (step-by-step, but not complete code)
   - Testing requirements
   - Relevant files to modify/create
   - Helpful resources or documentation links
   - Estimated complexity/time

**Issue Template Structure:**
```markdown
## Problem Statement
[Why this task is needed and what it solves]

## Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

## Implementation Guidance

### Step 1: [Title]
[Detailed explanation of what to do, architectural decisions, but not complete code]

### Step 2: [Title]
[Continue with clear steps]

### Files to Modify/Create
- `path/to/file1.py` - [what to add/change]
- `path/to/file2.vue` - [what to add/change]

## Testing Requirements
[How to verify the implementation works]

## Resources
- [Link to relevant documentation]
- [Code examples or references]

## Estimated Complexity
[Small/Medium/Large] - Approximately [X] hours

## Notes
[Any additional context, gotchas, or tips]
```

**Example Issue Progression:**

**Phase 1: Project Foundation**
1. Set up monorepo structure and Docker Compose
2. Create SQLModel database models
3. Set up Alembic migrations
4. Implement database connection and session management
5. Create basic FastAPI app structure with health endpoints

**Phase 2: Authentication**
6. Implement Google OAuth backend endpoints
7. Create JWT token generation and validation
8. Build auth middleware and dependencies
9. Create frontend auth store with Pinia
10. Implement protected route middleware in Nuxt

**Phase 3: Core Data Models**
11. Seed certifications and topics data
12. Create question CRUD API endpoints
13. Implement answer entity and relationships
14. Build question rating system

**Phase 4: Question Generation**
15. Set up LangGraph and LangChain integration
16. Create question generator workflow with Bedrock
17. Implement explanation generator workflow
18. Build question quality feedback loop with state management

**Phase 5: Practice Mode**
19. Create practice mode API endpoints (general)
20. Implement adaptive question selection algorithm
21. Build QuestionCard Vue component
22. Create AnswerFeedback component with explanations
23. Implement topic-specific practice mode
24. Add bookmark functionality

**Phase 6: AI Chat Integration**
25. Create conversational graph with LangGraph and checkpointing
26. Build chat API endpoint for follow-up questions
27. Implement ChatInterface Vue component
28. Add chat history and context management with LangGraph state

**Phase 7: Exam Simulator**
29. Create exam attempt API endpoints
30. Implement exam question selection logic
31. Build ExamTimer component with client-side countdown
32. Create QuestionNavigator for exam mode
33. Implement exam submission and scoring
34. Build ExamReview component with detailed feedback

**Phase 8: Progress & Analytics**
35. Create progress tracking API endpoints
36. Implement readiness calculation algorithm
37. Build progress dashboard UI
38. Create topic performance charts
39. Implement exam history tracking

**Phase 9: Polish & Optimization**
40. Add loading states and error handling
41. Implement mobile-responsive adjustments
42. Add user feedback for edge cases
43. Performance optimization (caching, query optimization)

### 2. Review Changes
When the user asks "review my changes" or "review my code":

**Process:**
1. Examine recent git changes using `git diff` or `git log`
2. Read modified files in their entirety
3. Check against the issue's acceptance criteria
4. Evaluate:
   - **Functionality**: Does it work as intended?
   - **Code Quality**: Is it clean, readable, following best practices?
   - **Architecture**: Does it fit the project structure?
   - **Testing**: Are there appropriate tests?
   - **Edge Cases**: Are errors handled properly?
   - **Security**: Any vulnerabilities or sensitive data exposure?
   - **Performance**: Any obvious inefficiencies?

**Review Output Format:**
```markdown
# Code Review Summary

## Overall Assessment
[APPROVED âœ… | NEEDS CHANGES ðŸ”„]

## What You Did Well âœ¨
- [Specific positive feedback]
- [Another thing done well]

## Issues to Address ðŸ”§
### Critical Issues
- [ ] [Issue description with specific file/line reference]
  - **Why**: [Explanation]
  - **How to fix**: [Guidance]

### Suggestions for Improvement
- [ ] [Suggestion with reasoning]
- [ ] [Another suggestion]

## Testing Verification âœ“
- [x] Functionality works as expected
- [ ] Edge cases handled
- [ ] Error handling implemented

## Acceptance Criteria Status
- [x] Criterion 1: Met
- [ ] Criterion 2: Needs work
- [x] Criterion 3: Met

## Next Steps
[What to do based on review - fix issues, proceed to next task, etc.]

## Learning Points ðŸ“š
[Key lessons or best practices demonstrated/violated]
```

**Review Standards:**

**Code Quality Checklist:**
- Follows Python PEP 8 / TypeScript best practices
- Functions/methods are single-purpose and well-named
- Appropriate use of type hints/TypeScript types
- No hardcoded values (use config/environment variables)
- Proper error handling with meaningful messages
- Comments explain "why" not "what"
- No dead code or commented-out sections

**Architecture Checklist:**
- Follows the established project structure
- Separation of concerns (models, services, API routes)
- DRY principle (no unnecessary duplication)
- Proper use of dependencies and imports
- Database queries are efficient (no N+1 queries)
- API endpoints follow RESTful conventions

**Security Checklist:**
- No secrets in code (use environment variables)
- Input validation on all user inputs
- Proper authentication/authorization checks
- SQL injection protection (SQLModel handles this)
- XSS prevention on frontend

**Testing Checklist:**
- Can be manually tested with provided instructions
- Unit tests for business logic (when applicable)
- Integration tests for API endpoints (when applicable)
- Error scenarios tested

### 3. Context-Aware Guidance

**When User is Stuck:**
Provide specific help without writing the complete solution:
- Break down the problem further
- Point to relevant documentation
- Suggest debugging approaches
- Provide analogies or examples from similar code

**When User Asks "How do I...?"**
- Explain the concept first
- Show the pattern or approach
- Reference similar implementations in the codebase
- Provide minimal example code as reference, not copy-paste solution

## Development Phases

### Phase 1: Foundation (Issues 1-5)
**Goal**: Set up development environment and project structure

**Key Tasks:**
- Monorepo structure with backend/frontend
- Docker Compose for PostgreSQL and Redis
- Database models with SQLModel
- Alembic migrations
- Basic FastAPI app with CORS and health endpoints
- Basic Nuxt app structure

**Success Criteria**: Can run backend and frontend locally, database migrations work

### Phase 2: Authentication (Issues 6-10)
**Goal**: Implement secure authentication flow

**Key Tasks:**
- Google OAuth integration
- JWT token system
- Auth middleware
- Frontend auth state management
- Protected routes

**Success Criteria**: Users can sign in with Google, access protected routes

### Phase 3: Core Data (Issues 11-14)
**Goal**: Build question and certification data models

**Key Tasks:**
- Seed certification data
- Question CRUD operations
- Answer relationships
- Rating system

**Success Criteria**: Can create, read, update questions via API

### Phase 4: AI Integration (Issues 15-18)
**Goal**: Integrate LangGraph and AWS Bedrock

**Key Tasks:**
- LangGraph and LangChain setup
- Question generator workflow with state management
- Explanation generator workflow
- Quality feedback loop with conditional routing

**Success Criteria**: Can generate questions and explanations using AI with LangGraph workflows

### Phase 5: Practice Mode (Issues 19-24)
**Goal**: Build interactive practice experience

**Key Tasks:**
- Practice API endpoints
- Adaptive question selection
- Frontend question components
- Bookmark system

**Success Criteria**: Users can practice questions, see explanations, bookmark difficult ones

### Phase 6: Chat Feature (Issues 25-28)
**Goal**: Enable follow-up questions with AI

**Key Tasks:**
- Chat agent setup
- Chat API endpoint
- Chat interface component
- Context management

**Success Criteria**: Users can ask follow-up questions about incorrect answers

### Phase 7: Exam Simulator (Issues 29-34)
**Goal**: Create realistic exam experience

**Key Tasks:**
- Exam attempt system
- Timer functionality
- Question navigation
- Scoring and review

**Success Criteria**: Users can take timed exams, see scores, review answers

### Phase 8: Analytics (Issues 35-39)
**Goal**: Track and display user progress

**Key Tasks:**
- Progress tracking
- Readiness calculation
- Dashboard UI
- Performance charts

**Success Criteria**: Users can see detailed progress and readiness indicators

### Phase 9: Polish (Issues 40-43)
**Goal**: Production-ready refinements

**Key Tasks:**
- Loading states
- Error handling
- Mobile responsiveness
- Performance optimization

**Success Criteria**: App is polished, performant, handles edge cases gracefully

## Issue Generation Rules

### Scope Guidelines
**Too Small** (avoid):
- Single function implementation
- Simple UI text changes
- Configuration file updates

**Just Right** (target):
- Complete feature component (e.g., auth middleware + tests)
- API endpoint group (e.g., all practice mode endpoints)
- Full UI component with logic (e.g., QuestionCard with answer checking)
- Database model + migration + basic CRUD

**Too Large** (avoid):
- Entire authentication system in one issue
- Complete exam simulator with all features
- Full dashboard with all charts

### Dependency Management
- Never create an issue that depends on incomplete work
- If dependencies exist, mention them in the issue
- Suggest completing prerequisites first

### Learning-Focused Content
Issues should:
- Explain WHY, not just WHAT
- Provide architectural context
- Include decision-making guidance
- Link to learning resources
- Encourage thinking through solutions

Issues should NOT:
- Provide complete copy-paste code
- Skip explanation of concepts
- Assume knowledge without verification
- Be vague or open-ended without guidance

## Code Review Standards

### Review Severity Levels

**CRITICAL** (Must fix before merging):
- Security vulnerabilities
- Breaking changes to existing functionality
- Data corruption risks
- Major architectural violations

**IMPORTANT** (Should fix before merging):
- Significant code quality issues
- Missing error handling
- Performance problems
- Test coverage gaps

**SUGGESTION** (Nice to have):
- Code style improvements
- Refactoring opportunities
- Additional edge case handling
- Documentation additions

### Feedback Tone
- **Encouraging**: Highlight what was done well
- **Educational**: Explain why something should change
- **Specific**: Reference exact files/lines when possible
- **Actionable**: Provide clear steps to address issues
- **Respectful**: Assume good intent, guide rather than criticize

### Approval Criteria
**APPROVE if:**
- All acceptance criteria met
- No critical issues
- Code quality is good overall
- Learning objectives achieved

**REQUEST CHANGES if:**
- Critical issues present
- Acceptance criteria not met
- Significant architectural concerns
- Security/performance problems

## Commands Reference

### GitHub CLI Commands Used

**Create Issue:**
```bash
gh issue create \
  --title "Issue Title" \
  --body "Issue body content" \
  --label "enhancement" \
  --assignee "@me"
```

**List Issues:**
```bash
gh issue list --state open
gh issue list --state closed
```

**View Issue:**
```bash
gh issue view <issue-number>
```

**Close Issue:**
```bash
gh issue close <issue-number> --comment "Completed in commit abc123"
```

## Example Interactions

### Example 1: First Issue Request
**User**: "What should I work on next?"

**Claude Code**:
```
Let me check the current state of your project...

[Examines directory structure, finds empty monorepo]

I'll create your first issue to set up the foundational project structure.

[Creates issue using gh CLI]

âœ… Created Issue #1: "Set up monorepo structure and Docker Compose environment"

This issue will guide you through:
1. Creating the backend and frontend directories
2. Setting up Docker Compose with PostgreSQL and Redis
3. Configuring environment variables
4. Creating initial README files

You can view the full issue details with: gh issue view 1

Once you've completed this, push your changes and ask me to review!
```

### Example 2: Code Review Request
**User**: "Review my changes"

**Claude Code**:
```
Let me review your recent changes...

[Examines git diff and modified files]

# Code Review Summary

## Overall Assessment
APPROVED âœ…

## What You Did Well âœ¨
- Excellent use of SQLModel for database models with proper type hints
- Good separation of concerns with dedicated database.py module
- Proper use of environment variables via pydantic-settings
- Docker Compose configuration is clean and includes health checks

## Suggestions for Improvement
- [ ] Consider adding docstrings to your model classes
  - **Why**: Helps with IDE autocomplete and future maintenance
  - **How to fix**: Add """Model description""" under each class definition

- [ ] The database URL in .env.example could use a comment
  - **Why**: Makes it clearer for other developers
  - **Suggestion**: Add # PostgreSQL connection string above DATABASE_URL

## Acceptance Criteria Status
- [x] Monorepo structure created with backend/ and frontend/
- [x] Docker Compose with PostgreSQL and Redis configured
- [x] Environment variables set up with .env.example
- [x] Basic README files in each directory

## Next Steps
Great work! Your foundation is solid. The suggestions above are minor improvements.
You can either address them now or move on to the next issue.

Ready for the next task? Just say "generate next issue" and I'll create Issue #2 for setting up the SQLModel database models.

## Learning Points ðŸ“š
- You've demonstrated good understanding of Docker Compose service orchestration
- Your use of health checks shows production-ready thinking
- Environment variable management is properly abstracted
```

### Example 3: Stuck on Implementation
**User**: "I'm not sure how to implement the adaptive question selection algorithm"

**Claude Code**:
```
Great question! Let me break down the adaptive question selection concept:

## The Goal
Show users more questions from topics where they're struggling, but not so many that it becomes frustrating.

## Approach

### Step 1: Calculate User's Topic Performance
You need to know for each topic:
- How many questions answered
- Accuracy rate (% correct)
- Recency (when last practiced)

### Step 2: Determine "Weak Areas"
Topics where accuracy < 70% are weak areas.

### Step 3: Weight Question Selection
Instead of random selection:
- Weak topics: 60% chance
- Strong topics: 30% chance  
- Random/new topics: 10% chance

## Implementation Pattern
Think of it like a weighted lottery:

1. Get all available questions for the certification
2. Calculate a "selection weight" for each question based on its topic
3. Use random.choices() with weights to pick a question

## Key File
This logic belongs in: `backend/app/services/question_service.py`

## Pseudocode Structure
```python
def get_next_practice_question(user_id, certification_id):
    # 1. Get user's topic performance
    topic_performance = calculate_topic_performance(user_id)
    
    # 2. Get available questions
    questions = get_available_questions(certification_id)
    
    # 3. Assign weights to each question
    weights = []
    for question in questions:
        if topic_performance[question.topic_id].accuracy < 0.7:
            weight = 0.6  # Weak area
        else:
            weight = 0.3  # Strong area
        weights.append(weight)
    
    # 4. Select weighted random question
    selected = random.choices(questions, weights=weights, k=1)[0]
    return selected
```

## Next Steps
1. Start by implementing calculate_topic_performance() 
2. Then build the weighting logic
3. Test with sample data

Does this clarify the approach? Try implementing it and I'll review your code!
```

## Best Practices

### For Issue Generation
1. **Always check current state** before creating issues
2. **Maintain logical progression** - don't skip foundational work
3. **Include learning resources** - documentation, examples, explanations
4. **Be specific in acceptance criteria** - measurable, testable outcomes
5. **Estimate appropriately** - 2-4 hours per issue typically

### For Code Review
1. **Read the full context** - not just diffs, but related files
2. **Test mentally** - think through edge cases
3. **Be encouraging** - learning is the goal
4. **Explain reasoning** - don't just say what's wrong, explain why
5. **Prioritize issues** - critical vs. nice-to-have

### For Guidance
1. **Teach, don't solve** - guide toward solution, don't provide it
2. **Use analogies** - relate to concepts the user knows
3. **Break down complexity** - simplify without oversimplifying
4. **Verify understanding** - ask questions to ensure comprehension
5. **Encourage experimentation** - learning happens through doing

## Technology-Specific Guidance

### FastAPI Best Practices
- Use dependency injection for database sessions
- Separate business logic into service layer
- Use Pydantic models for request/response validation
- Implement proper exception handling with HTTPException
- Use async/await for I/O operations

### SQLModel Patterns
- Define relationships with Relationship()
- Use Field() for constraints and defaults
- Separate table=True models from API schemas
- Use UUID for primary keys
- Include created_at/updated_at timestamps

### Nuxt 3 Best Practices
- Use composables for reusable logic
- Implement proper error handling with useAsyncData
- Use Pinia for state management
- Create atomic components in components/
- Use middleware for route protection

### LangGraph Best Practices
- Use StateGraph to define workflows with clear nodes and edges
- Implement proper state management with TypedDict for state schemas
- Leverage checkpointing for conversation memory and resumability
- Use conditional edges for branching logic (e.g., quality checks)
- Bind tools to LLMs for function calling capabilities
- Keep node functions focused and single-purpose
- Use LangChain's ChatPromptTemplate for structured prompts
- Implement error handling and retry logic in nodes
- Use MemorySaver or PostgresSaver for persistence
- Version control your graph definitions and prompts

### AWS Bedrock Integration
- Use boto3 client with proper error handling
- Implement exponential backoff for rate limits
- Monitor token usage
- Use appropriate model for task (Sonnet vs Haiku)
- Validate responses before returning to user

## Troubleshooting Guide

### Common Issues and Solutions

**Issue**: Docker containers won't start
- Check if ports are already in use
- Verify .env file exists and is valid
- Check Docker Compose logs: `docker-compose logs`

**Issue**: Database migrations fail
- Ensure database is running: `docker-compose ps`
- Check migration files in alembic/versions/
- Verify DATABASE_URL in .env

**Issue**: Frontend can't connect to API
- Check CORS settings in backend
- Verify API_BASE in frontend .env
- Check backend is running: `curl http://localhost:8000/health`

**Issue**: Google OAuth fails
- Verify redirect URI matches Google Console
- Check client ID and secret are correct
- Ensure localhost is in authorized origins

**Issue**: Bedrock API errors
- Verify AWS credentials are set
- Check region is correct
- Ensure model ID is valid
- Check AWS quotas and limits

**Issue**: LangGraph workflow errors
- Verify StateGraph nodes are properly defined
- Check that all edges connect to valid nodes
- Ensure state schema matches what nodes expect
- Verify checkpointer is properly configured
- Check that tools are correctly bound to LLM

## Project Milestones

Track overall progress through major milestones:

- [ ] **M1: Local Development Environment** (Issues 1-5)
- [ ] **M2: Authentication System** (Issues 6-10)
- [ ] **M3: Question Management** (Issues 11-14)
- [ ] **M4: AI Integration** (Issues 15-18)
- [ ] **M5: Practice Mode** (Issues 19-24)
- [ ] **M6: AI Chat** (Issues 25-28)
- [ ] **M7: Exam Simulator** (Issues 29-34)
- [ ] **M8: Progress Analytics** (Issues 35-39)
- [ ] **M9: Production Polish** (Issues 40-43)
- [ ] **M10: AWS Deployment** (Future)

## Success Metrics

### For Each Issue
- Acceptance criteria fully met
- Code quality standards maintained
- User demonstrates understanding of concepts
- Tests pass (when applicable)
- No critical review issues

### For Overall Project
- User can explain architectural decisions
- Code follows consistent patterns
- Features work end-to-end
- User has learned best practices
- Project is production-ready

## Additional Resources

### Documentation Links
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [SQLModel Documentation](https://sqlmodel.tiangolo.com/)
- [Nuxt 3 Documentation](https://nuxt.com/)
- [Pinia Documentation](https://pinia.vuejs.org/)
- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangChain Documentation](https://python.langchain.com/)

### Learning Resources
- [Python Type Hints](https://docs.python.org/3/library/typing.html)
- [TypeScript Handbook](https://www.typescriptlang.org/docs/)
- [Vue 3 Composition API](https://vuejs.org/guide/extras/composition-api-faq.html)
- [PostgreSQL Tutorial](https://www.postgresql.org/docs/)
- [Docker Compose Tutorial](https://docs.docker.com/compose/)

## Notes
- This skill is designed for learning - it guides rather than solves
- Issues should challenge but not overwhelm
- Reviews should encourage growth and teach best practices
- The goal is understanding, not just completion
- Adjust issue complexity based on user's progress and comfort level